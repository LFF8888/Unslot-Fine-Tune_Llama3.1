2024å¹´7æœˆ17æ—¥ï¼Œæ˜ŸæœŸä¸‰ï¼Œä½œè€…ï¼š[adebisi_oluwatomiwa878](https://lablab.ai/u/@adebisi_oluwatomiwa878)

![Llama 3 å¾®è°ƒï¼šæŒæ¡ AI é¡¹ç›®çš„å®šåˆ¶åŒ–](https://lablab.ai/_next/image?url=https%3A%2F%2Fimagedelivery.net%2FK11gkZF3xaVyYzFESMdWIQ%2Fabfb22d8-6222-4698-a302-0efcc88a1000%2Ffull&w=3840&q=80)

## ğŸš€ Llama 3 å¾®è°ƒï¼šæŒæ¡ AI é¡¹ç›®çš„å®šåˆ¶åŒ–

æ¬¢è¿é˜…è¯»æœ¬æ•™ç¨‹ï¼åœ¨è¿™é‡Œï¼Œæˆ‘å°†æŒ‡å¯¼ä½ å¦‚ä½•ä½¿ç”¨çœŸå®ä¸–ç•Œçš„æ•°æ®é›†å¯¹Llama 3æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡æœ¬æ•™ç¨‹çš„å­¦ä¹ ï¼Œä½ å°†èƒ½å¤Ÿåœ¨AIé»‘å®¢é©¬æ‹‰æ¾åŠå…¶ä»–ä»¤äººå…´å¥‹çš„é¡¹ç›®ä¸­åº”ç”¨æ‰€å­¦çŸ¥è¯†ã€‚

### ç›®æ ‡ ğŸ“‹

æœ¬æ•™ç¨‹å°†æ¶µç›–ä»¥ä¸‹å†…å®¹ï¼š

- ä½¿ç”¨å¯å®šåˆ¶çš„æ•°æ®é›†å¯¹Llama 3è¿›è¡Œä»»åŠ¡å¾®è°ƒçš„è¿‡ç¨‹ã€‚
- åˆ©ç”¨Unslothå®ç°çš„Llama 3è¿›è¡Œé«˜æ•ˆè®­ç»ƒã€‚
- åˆ©ç”¨Hugging Faceçš„å·¥å…·è¿›è¡Œæ¨¡å‹å¤„ç†å’Œæ•°æ®é›†ç®¡ç†ã€‚
- æ ¹æ®ä½ çš„å…·ä½“éœ€æ±‚è°ƒæ•´å¾®è°ƒè¿‡ç¨‹ï¼Œä½¿Llama 3å¯ä»¥é€‚åº”ä»»ä½•ä»»åŠ¡ã€‚

### å…ˆå†³æ¡ä»¶ ğŸ› ï¸

- åŸºç¡€çš„TransformerçŸ¥è¯†
- ç†Ÿæ‚‰Pythonç¼–ç¨‹
- è®¿é—®Google Colab
- åŸºæœ¬çš„æ¨¡å‹å¾®è°ƒçŸ¥è¯†

## è®¾ç½®ç¯å¢ƒ ğŸ–¥ï¸

### Google Colab âš™ï¸

é¦–å…ˆï¼Œæ‰“å¼€[Google Colab](https://colab.research.google.com/)å¹¶åˆ›å»ºä¸€ä¸ªæ–°çš„notebookã€‚ç¡®ä¿å¯ç”¨GPUæ”¯æŒä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚ä½ å¯ä»¥é€šè¿‡å¯¼èˆªè‡³ `Edit > Notebook settings` å¹¶é€‰æ‹© `T4 GPU` ä½œä¸ºç¡¬ä»¶åŠ é€Ÿå™¨æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç¡®ä¿é€‰æ‹©T4 GPUä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚

### å®‰è£…ä¾èµ– ğŸ“¦

åœ¨ä½ çš„Colab notebookä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…å¿…è¦çš„åº“ï¼š

```python
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes
```

### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ ğŸ“š

æˆ‘ä»¬å°†ä½¿ç”¨Unslothå®ç°çš„Llama 3ï¼Œè¯¥å®ç°é’ˆå¯¹æ›´å¿«çš„è®­ç»ƒå’Œæ¨ç†è¿›è¡Œäº†ä¼˜åŒ–ã€‚

> **æ³¨æ„:** å¦‚æœä½ ä½¿ç”¨çš„æ˜¯æ¥è‡ªHugging Faceçš„å—é™æ¨¡å‹ï¼Œéœ€è¦åœ¨ `FastLanguageModel.from_pretrained` ä¸­æ·»åŠ å­—æ®µ "token"ï¼Œå¹¶å¡«å†™ä½ çš„Hugging Faceè®¿é—®ä»¤ç‰Œã€‚

```python
from unsloth import FastLanguageModel
import torch

max_seq_length = 2048  # å¯¹äºLlama 3ï¼Œä½ å¯ä»¥é€‰æ‹©ä»»æ„å€¼ï¼Œæœ€é«˜å¯è¾¾8000
dtype = None  # è‡ªåŠ¨æ£€æµ‹ã€‚Tesla T4, V100 é€‰æ‹©Float16, Ampere+é€‰æ‹©Bfloat16
load_in_4bit = True  # ä½¿ç”¨4bité‡åŒ–ä»¥å‡å°‘å†…å­˜ä½¿ç”¨é‡ã€‚å¯ä»¥é€‰æ‹©Falseã€‚

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-bnb-4bit",
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
    # token="YOUR_HUGGINGFACE_ACCESS_TOKEN"  # ä½¿ç”¨å—é™æ¨¡å‹æ—¶æ·»åŠ æ­¤è¡Œ
)
```

### å‡†å¤‡æ•°æ®é›† ğŸ“Š

é¦–å…ˆï¼Œå°†ä½ çš„ `dataset.json` æ–‡ä»¶ä¸Šä¼ åˆ°Google Colabã€‚ä»¥ä¸‹æ˜¯ç”¨äºè®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹çš„æ•°æ®é›†ç¤ºä¾‹ï¼š

```json
[
  {
    "instruction": "åˆ†ç±»ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿã€‚",
    "input": "æˆ‘å–œæ¬¢è¿™ä¸ªäº§å“çš„æ–°åŠŸèƒ½ï¼",
    "output": "ç§¯æ"
  },
  {
    "instruction": "åˆ†ç±»ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿã€‚",
    "input": "å¤©æ°”è¿˜å¯ä»¥ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„ã€‚",
    "output": "ä¸­æ€§"
  },
  {
    "instruction": "åˆ†ç±»ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿã€‚",
    "input": "æˆ‘å¯¹è¿™ä¸ªæœåŠ¡æ„Ÿåˆ°éå¸¸å¤±æœ›ã€‚",
    "output": "æ¶ˆæ"
  },
  {
    "instruction": "åˆ†ç±»ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿã€‚",
    "input": "è¿™éƒ¨ç”µå½±å¾ˆæ£’ï¼Œå¾ˆåˆºæ¿€ï¼",
    "output": "ç§¯æ"
  },
  {
    "instruction": "åˆ†ç±»ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿã€‚",
    "input": "æˆ‘ä¸ä»‹æ„ç­‰å¾…ï¼Œè¿™æ²¡ä»€ä¹ˆå¤§ä¸äº†çš„ã€‚",
    "output": "ä¸­æ€§"
  },
  {
    "instruction": "åˆ†ç±»ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿã€‚",
    "input": "é£Ÿç‰©å¾ˆç³Ÿç³•ï¼Œæ¯«æ— å‘³é“ã€‚",
    "output": "æ¶ˆæ"
  },
  {
    "instruction": "åˆ†ç±»ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿã€‚",
    "input": "ä»Šå¤©åœ¨å…¬å›­é‡Œç©å¾—å¾ˆå¼€å¿ƒï¼",
    "output": "ç§¯æ"
  },
  {
    "instruction": "åˆ†ç±»ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿã€‚",
    "input": "è¿™æœ¬ä¹¦å¾ˆæ— èŠï¼ŒèŠ‚å¥å¾ˆæ…¢ã€‚",
    "output": "æ¶ˆæ"
  },
  {
    "instruction": "åˆ†ç±»ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿã€‚",
    "input": "è¿™åªæ˜¯æ™®é€šçš„ä¸€å¤©ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„ã€‚",
    "output": "ä¸­æ€§"
  },
  {
    "instruction": "åˆ†ç±»ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿã€‚",
    "input": "å®¢æœéå¸¸æœ‰å¸®åŠ©ã€‚",
    "output": "ç§¯æ"
  }
]
```

æ¥ä¸‹æ¥ï¼Œå®šä¹‰ä¸æ•°æ®é›†ç»“åˆä½¿ç”¨çš„æç¤ºæ¨¡æ¿ï¼Œç„¶åä»ä¸Šä¼ çš„ `dataset.json` æ–‡ä»¶ä¸­åŠ è½½æ•°æ®é›†ï¼š

```python
from datasets import load_dataset

fine_tuned_prompt = """ä»¥ä¸‹æ˜¯æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œæ­é…çš„è¾“å…¥æä¾›äº†è¿›ä¸€æ­¥çš„ä¸Šä¸‹æ–‡ã€‚è¯·ç¼–å†™ä¸€ä¸ªé€‚å½“å®Œæˆè¯·æ±‚çš„å“åº”ã€‚

### æŒ‡ä»¤:
{}

### è¾“å…¥:
{}

### å“åº”:
{}"""

EOS_TOKEN = tokenizer.eos_token  # å¿…é¡»æ·»åŠ EOS_TOKEN
def formatting_prompts_func(prompt_dict):
    instructions = prompt_dict["instruction"]
    inputs       = prompt_dict["input"]
    outputs      = prompt_dict["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        # å¿…é¡»æ·»åŠ EOS_TOKENï¼Œå¦åˆ™ç”Ÿæˆè¿‡ç¨‹å°†æ°¸è¿œä¸ä¼šåœæ­¢ï¼
        text = fine_tuned_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }
pass

# ä»æœ¬åœ°JSONæ–‡ä»¶åŠ è½½æ•°æ®é›†
dataset = load_dataset('json', data_files='dataset.json', split='train')
dataset = dataset.map(formatting_prompts_func, batched = True)
```

### æ¨¡å‹å¾®è°ƒ ğŸ”§

æˆ‘ä»¬å°†ä½¿ç”¨ **LoRA (ä½ç§©é€‚é…)** æ¥é«˜æ•ˆåœ° **å¾®è°ƒ** æ¨¡å‹ã€‚LoRAé€šè¿‡åœ¨Transformeræ¶æ„çš„æ¯ä¸€å±‚ä¸­æ’å…¥å¯è®­ç»ƒçš„ä½ç§©çŸ©é˜µæ¥é€‚åº”å¤§å‹æ¨¡å‹ã€‚

```python
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # é€‰æ‹©ä»»æ„å¤§äº0çš„æ•°ï¼å»ºè®®å€¼ä¸º8, 16, 32, 64, 128
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,  # æ”¯æŒä»»æ„å€¼ï¼Œä½†=0ä¸ºä¼˜åŒ–è®¾ç½®
    bias="none",  # æ”¯æŒä»»æ„å€¼ï¼Œä½†="none"ä¸ºä¼˜åŒ–è®¾ç½®
    use_gradient_checkpointing="unsloth",  # Trueæˆ–â€œunslothâ€å¯å‡å°‘30%çš„VRAMä½¿ç”¨é‡
)
```

### å‚æ•°è¯´æ˜ ğŸ“

- **r:** ä½ç§©è¿‘ä¼¼çš„ç§©ï¼Œè®¾ç½®ä¸º16åœ¨æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡ã€‚
- **target_modules:** æŒ‡å®šLoRAåº”ç”¨çš„æ¨¡å—ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹çš„å…³é”®éƒ¨åˆ†ã€‚
- **lora_alpha:** LoRAæƒé‡çš„ç¼©æ”¾å› å­ï¼Œè®¾ç½®ä¸º16ä»¥ç¡®ä¿è®­ç»ƒç¨³å®šæ€§ã€‚
- **lora_dropout:** åº”ç”¨äºLoRAå±‚çš„dropoutç‡ï¼Œè®¾ç½®ä¸º0è¡¨ç¤ºæ— dropoutã€‚
- **bias:** æŒ‡å®šå¦‚ä½•å¤„ç†åç½®é¡¹ï¼Œè®¾ç½®ä¸ºâ€œnoneâ€è¡¨ç¤ºä¸è®­ç»ƒåç½®é¡¹ã€‚
- **use_gradient_checkpointing:** é€šè¿‡å­˜å‚¨ä¸­é—´æ¿€æ´»å€¼æ¥å‡å°‘å†…å­˜ä½¿ç”¨é‡ã€‚

### è®­ç»ƒ ğŸ‹ï¸

æˆ‘ä»¬å°†ä½¿ç”¨Hugging Faceçš„SFTTrainerè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

```python
from transformers import TrainingArguments
from trl import SFTTrainer
from unsloth import is_bfloat16_supported

training_args = TrainingArguments(
    output_dir="./results",
    # num_train_epochs=1,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
    gradient_accumulation_steps = 4,
    warmup_steps = 5,
    max_steps = 60,
    learning_rate = 2e-4,
    fp16 = not is_bfloat16_supported(),
    bf16 = is_bfloat16_supported(),
)

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    dataset_text_field="text",
    max_seq_length = max_seq_length,
)

trainstats =

 trainer.train()
```

`TrainingArguments` ä½¿ç”¨çš„å‚æ•°ï¼š

- **output_dir:** ä¿å­˜è®­ç»ƒæ¨¡å‹å’Œæ£€æŸ¥ç‚¹çš„ç›®å½•ã€‚è¿™å¯¹äºæ¢å¤è®­ç»ƒå’Œå…±äº«æ¨¡å‹è‡³å…³é‡è¦ã€‚
- **per_device_train_batch_size:** åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šä½¿ç”¨çš„è®­ç»ƒæ‰¹é‡å¤§å°ã€‚è¿™ä¼šå½±å“å†…å­˜ä½¿ç”¨é‡å’Œè®­ç»ƒé€Ÿåº¦ã€‚
- **save_steps:** æ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹ã€‚è¿™æœ‰åŠ©äºåœ¨è®­ç»ƒä¸­æ–­çš„æƒ…å†µä¸‹ä»æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚
- **save_total_limit:** ä¿ç•™çš„æœ€å¤§æ£€æŸ¥ç‚¹æ•°é‡ã€‚æ—§çš„æ£€æŸ¥ç‚¹å°†è¢«åˆ é™¤ï¼Œè¿™æœ‰åŠ©äºç®¡ç†ç£ç›˜ç©ºé—´ã€‚
- **gradient_accumulation_steps:** åœ¨æ‰§è¡Œåå‘ä¼ æ’­ä¹‹å‰ç´¯ç§¯æ¢¯åº¦çš„æ­¥æ•°ã€‚è¿™å¯¹æ— æ³•å®¹çº³è¾ƒå¤§æ‰¹é‡å¤§å°çš„å¤§å‹æ¨¡å‹éå¸¸æœ‰ç”¨ã€‚
- **warmup_steps:** æ‰§è¡Œå­¦ä¹ ç‡é¢„çƒ­çš„æ­¥æ•°ã€‚è¿™æœ‰åŠ©äºç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚
- **max_steps:** æ€»è®­ç»ƒæ­¥æ•°ã€‚è¾¾åˆ°æ­¤æ­¥æ•°åè®­ç»ƒå°†åœæ­¢ã€‚
- **learning_rate:** ç”¨äºè®­ç»ƒçš„å­¦ä¹ ç‡ã€‚è¿™æ§åˆ¶äº†æ¨¡å‹æƒé‡æ›´æ–°çš„å¤§å°ã€‚
- **fp16:** æ˜¯å¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨16ä½ï¼ˆåŠç²¾åº¦ï¼‰æµ®ç‚¹æ•°ï¼Œè¿™å¯ä»¥å‡å°‘å†…å­˜ä½¿ç”¨é‡å¹¶åŠ é€Ÿæ”¯æŒè¯¥åŠŸèƒ½çš„GPUä¸Šçš„è®­ç»ƒã€‚
- **bf16:** æ˜¯å¦ä½¿ç”¨bfloat16ï¼ˆè„‘æµ®ç‚¹ï¼‰ç²¾åº¦ï¼Œè¿™å¯¹æŸäº›ç¡¬ä»¶å¦‚TPUæœ‰ç›Šã€‚

`SFTTrainer` ä½¿ç”¨çš„å‚æ•°ï¼š

- **model:** è¦è®­ç»ƒçš„æ¨¡å‹ã€‚
- **args:** å®šä¹‰è®­ç»ƒé…ç½®çš„TrainingArgumentsã€‚
- **train_dataset:** ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚
- **tokenizer:** ç”¨äºå¤„ç†æ•°æ®çš„åˆ†è¯å™¨ã€‚å®ƒå¯¹äºå°†æ–‡æœ¬è½¬æ¢ä¸ºè¾“å…¥å¼ é‡è‡³å…³é‡è¦ã€‚
- **dataset_text_field:** æ•°æ®é›†ä¸­åŒ…å«ç”¨äºè®­ç»ƒçš„æ–‡æœ¬çš„å­—æ®µåç§°ã€‚
- **max_seq_length:** è¾“å…¥æ¨¡å‹çš„åºåˆ—æœ€å¤§é•¿åº¦ã€‚è¶…è¿‡æ­¤é•¿åº¦çš„åºåˆ—å°†è¢«æˆªæ–­ã€‚

### ä½¿ç”¨å¾®è°ƒæ¨¡å‹ ğŸ§ 

ç°åœ¨æ¨¡å‹å·²ç»è®­ç»ƒå®Œæ¯•ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä¸€äº›æ ·æœ¬è¾“å…¥æ¥æµ‹è¯•æƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼š

- æ¨ç†æ˜¯ä½¿ç”¨è®­ç»ƒæ¨¡å‹å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹çš„è¿‡ç¨‹ã€‚

```python
FastLanguageModel.for_inference(model) # å¯ç”¨åŸç”Ÿ2å€é€Ÿæ¨ç†
inputs = tokenizer(
[
    fine_tuned_prompt.format(
        "åˆ†ç±»ä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿã€‚", # æŒ‡ä»¤
        "æˆ‘ä¸å–œæ¬¢åœ¨é›¨ä¸­è¸¢è¶³çƒ", # è¾“å…¥
        "", # è¾“å‡º - ä¿æŒç©ºç™½ä»¥è¿›è¡Œç”Ÿæˆï¼
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
outputs = tokenizer.decode(outputs[0])
print(outputs)
```

### ä¿å­˜å’Œå…±äº«æ¨¡å‹ ğŸ’¾

ä¿å­˜å¾®è°ƒæ¨¡å‹æœ‰ä¸¤ç§æ–¹æ³•ï¼š

#### æœ¬åœ°ä¿å­˜æ¨¡å‹

```python
model.save_pretrained("path/to/save")
tokenizer.save_pretrained("path/to/save")
```

#### å°†æ¨¡å‹ä¿å­˜åˆ°Hugging Face Hubï¼ˆåœ¨çº¿ï¼‰

```python
model.push_to_hub("your_username/your_model_name", token = "YOUR_HUGGINGFACE_ACCESS_TOKEN")
tokenizer.push_to_hub("your_username/your_model_name", token = "YOUR_HUGGINGFACE_ACCESS_TOKEN")
```

## ç»“è®º ğŸ‰

é€šè¿‡è¿™äº›æ­¥éª¤ï¼Œä½ åº”è¯¥å·²ç»æŒæ¡äº†å¦‚ä½•ä¸ºå„ç§ä»»åŠ¡å¾®è°ƒLlama 3æ¨¡å‹ã€‚æŒæ¡è¿™äº›æŠ€æœ¯åï¼Œä½ å°†èƒ½å¤Ÿæ ¹æ®è‡ªå·±çš„éœ€æ±‚è°ƒæ•´æ¨¡å‹ï¼Œä½¿ä½ èƒ½å¤Ÿæ›´åŠ é«˜æ•ˆå’Œç²¾ç¡®åœ°å¤„ç†AIé¡¹ç›®ã€‚ç¥ä½ çš„å¾®è°ƒå’ŒAIé¡¹ç›®é¡ºåˆ©ï¼ğŸš€